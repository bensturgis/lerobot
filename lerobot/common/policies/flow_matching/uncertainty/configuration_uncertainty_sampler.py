import warnings
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Sequence

# TODO: Update description
"""
Explanation of shared uncertainy sampler attributes:
- num_action_seq_samples: Number of candidate action sequences to sample.
- scoring_metric:
    - metric_type: Which uncertainty metric to use.
        - "likelihood": Negative log-likelihood of the action sequence.
        - "mode_distance": Proxy "distance-from-mode" score computed by averaging 
            (1 - t) * ‖v(t)‖ of the scorer's velocity at the final sampled action sequence
            across the specified evaluation times.
        - "terminal_vel_norm": Average L2-norm of the velocity field evaluated only for the
            final state of the ODE at the specified evaluation times.
        - "inter_vel_diff": Average L2 norm of the velocity differences between the
            scorer and sampler ODE over intermediate evaluation points of the ODE.
    - likelihood_ode_solver_cfg: When scoring with "likelihood", these are the ODE-solver settings
        used to compute the log-probability.
        - exact_divergence: Whether to compute the exact divergence or use the Hutchinson
            trace estimator when computing the log-likelihood for an action sequence sample.
    - velocity_eval_times: When we score based on the velocity field, these are the time-points 
        we evaluate the velocity field to compute the metrics.
- ensemble_model_path: Path to the pretrained flow matching model that serves as the “scorer”
    for ensembling methods. The scorer is the model used to compute log-likelihoods for the
    action sequences generated by the primary “sampler” flow-matching model. If None and
    cross-likelihood sampling is requested, an error will be raised.
- laplace_scope: Which layer(s) to place the Laplace posterior on for Laplace based
    uncertainty sampler.
    - "velocity_last": The final layer of the flow matching velocity model.
    - "rgb_last": the final layer of the RGB encoder.
    - "both": jointly on both layers.
"""

def validate_metric(field_name: str, metric: str, allowed: Sequence[str]) -> None:
    """
    Raise ValueError if metric is not one of allowed.
    """
    if metric not in allowed:
        allowed_list = ", ".join(f"'{m}'" for m in allowed)
        raise ValueError(
            f"{field_name!r} must be one of: {allowed_list}. Got {metric!r}."
        )

def process_velocity_eval_times(
    scoring_metric: str,
    vel_eval_times: Sequence[float],
) -> List[float]:
    """
    Validate and possibly modify vel_eval_times.

    Ensures every t satisfies 0.0 ≤ t < 1.0.
    If scoring_metric == "inter_vel_diff" and vel_eval_times does not already
    start with 0.0, prepends 0.0 and issues a warning.
    """
    for t in vel_eval_times:
        if not (0.0 <= t < 1.0):
            raise ValueError(
                f"'velocity_eval_times' entries must satisfy 0.0 ≤ t < 1.0; "
                f"got {vel_eval_times}"
            )

    if scoring_metric == "inter_vel_diff":
        needs_zero = len(vel_eval_times) == 0 or vel_eval_times[0] != 0.0
        if needs_zero:
            warnings.warn(
                "scoring_metric 'inter_vel_diff' requires velocity_eval_times "
                "to start with 0.0; prepending 0.0 automatically.",
                stacklevel=2,
            )
            vel_eval_times = [0.0, *vel_eval_times]

    return vel_eval_times

@dataclass
class LikelihoodODESolverConfig:
    """Configuration for the ODE solver used in likelihood estimation."""
    method: str = "euler"
    atol: float | None = None
    rtol: float | None = None
    exact_divergence: bool = False

@dataclass
class ScoringMetricConfig:
    """
    Scoring settings shared by all uncertainty samplers.
    """
    metric_type: str = "likelihood"
    likelihood_ode_solver_cfg: LikelihoodODESolverConfig = field(
        default_factory=LikelihoodODESolverConfig
    )
    velocity_eval_times: tuple | None = None

    def __post_init__(self) -> None:
        # Set velocity_eval_times based on metric
        if self.velocity_eval_times is None:
            if self.metric_type == "inter_vel_diff":
                self.velocity_eval_times = tuple(i * 0.1 for i in range(0, 10)) + (0.95,)
            elif self.metric_type == "mode_distance":
                self.velocity_eval_times = (0.0, 0.1, 0.2)
            elif self.metric_type == "terminal_vel_norm":
                self.velocity_eval_times = (0.7, 0.75, 0.8)
            else:
                self.velocity_eval_times = ()

        self.velocity_eval_times = process_velocity_eval_times(
            scoring_metric=self.metric_type,
            vel_eval_times=self.velocity_eval_times,
        )

# Sub-configs for each uncertainty sampler.
@dataclass
class ComposedCrossBayesianSamplerConfig:
    scorer_type: str = "ensemble"
    num_action_seq_samples: int = 5
    scoring_metric: ScoringMetricConfig = field(default_factory=ScoringMetricConfig)
    
    # Parameters for the ensemble model
    ensemble_model_path: str | Path | None = None

    # Parameters for the Laplace approximation calibration dataloader
    laplace_scope: str = "both"
    calib_fraction: float = 1.0
    batch_size: int = 1

    def __post_init__(self):
        # Validate scorer type
        allowed_scorer_types = {"ensemble", "laplace"}
        if self.scorer_type not in allowed_scorer_types:
            raise ValueError(
                f"CrossBayesianSamplerConfig.scorer_type must be one of "
                f"{sorted(allowed_scorer_types)}, got {self.scorer_type}."
            )
        
        # Validate Laplace scope
        allowed_scopes = {"velocity_last", "rgb_last", "both"}
        if self.laplace_scope not in allowed_scopes:
            raise ValueError(
                f"CrossLaplaceSamplerConfig.laplace_scope must be one of "
                f"{sorted(allowed_scopes)}, got {self.laplace_scope!r}."
            )
        
        # Validate scoring metric
        validate_metric(
            field_name="CrossEnsembleSamplerConfig.scoring_metric",
            metric=self.scoring_metric.metric_type,
            allowed=(
                "inter_vel_diff", "likelihood", "mode_distance", "terminal_vel_norm",
            ),
        )

@dataclass
class CrossBayesianSamplerConfig:
    scorer_type: str = "ensemble"
    num_action_seq_samples: int = 5
    scoring_metric: ScoringMetricConfig = field(default_factory=ScoringMetricConfig)

    # Parameters for the ensemble model
    ensemble_model_path: str | Path | None = None

    # Parameters for the Laplace approximation calibration dataloader
    laplace_scope: str = "both"
    calib_fraction: float = 1.0
    batch_size: int = 1

    def __post_init__(self):
        # Validate scorer type
        allowed_scorer_types = {"ensemble", "laplace"}
        if self.scorer_type not in allowed_scorer_types:
            raise ValueError(
                f"CrossBayesianSamplerConfig.scorer_type must be one of "
                f"{sorted(allowed_scorer_types)}, got {self.scorer_type}."
            )

        # Validate Laplace scope
        allowed_scopes = {"velocity_last", "rgb_last", "both"}
        if self.laplace_scope not in allowed_scopes:
            raise ValueError(
                f"CrossLaplaceSamplerConfig.laplace_scope must be one of "
                f"{sorted(allowed_scopes)}, got {self.laplace_scope}."
            )
        
        # Validate scoring metric
        validate_metric(
            field_name="CrossEnsembleSamplerConfig.scoring_metric",
            metric=self.scoring_metric.metric_type,
            allowed=(
                 "inter_vel_diff", "likelihood", "mode_distance", "terminal_vel_norm",
            ),
        )

@dataclass
class ComposedSequenceSamplerConfig:
    num_action_seq_samples: int = 5
    scoring_metric: ScoringMetricConfig = field(default_factory=ScoringMetricConfig)
    
    def __post_init__(self):
        validate_metric(
            field_name="CrossEnsembleSamplerConfig.scoring_metric",
            metric=self.scoring_metric.metric_type,
            allowed=(
                 "inter_vel_diff", "likelihood", "mode_distance", "terminal_vel_norm",
            ),
        )

@dataclass
class EntropySamplerConfig:
    num_action_seq_samples: int = 5

@dataclass
class UncertaintySamplerConfig:
    type: str = "composed_cross_bayesian"
    composed_cross_bayesian_sampler: ComposedCrossBayesianSamplerConfig = field(
        default_factory=ComposedCrossBayesianSamplerConfig
    )
    composed_sequence_sampler: ComposedSequenceSamplerConfig = field(
        default_factory=ComposedSequenceSamplerConfig
    )
    cross_bayesian_sampler: CrossBayesianSamplerConfig = field(
        default_factory=CrossBayesianSamplerConfig
    )
    entropy_sampler: EntropySamplerConfig = field(
        default_factory=EntropySamplerConfig
    )

    @property
    def active_config(self):
        """
        Return the sub-config matching self.type.
        """
        mapping = {
            "composed_cross_bayesian": self.composed_cross_bayesian_sampler,
            "composed_sequence": self.composed_sequence_sampler,
            "cross_bayesian": self.cross_bayesian_sampler,
            "entropy": self.entropy_sampler,
        }
        try:
            return mapping[self.type]
        except KeyError as err:
            valid = ", ".join(sorted(mapping.keys()))
            raise ValueError(
                f"Unknown uncertainty sampler type {self.type}. "
                f"Expected one of: {valid}."
            ) from err